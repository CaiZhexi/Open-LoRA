#!/usr/bin/env python3
"""
äº¤äº’å¼é«˜ä¸­æ•™å­¦AIåŠ©æ‰‹
è‡ªåŠ¨æ£€æµ‹modelsç›®å½•ä¸‹çš„æ‰€æœ‰æ¨¡å‹ï¼Œæä¾›é€‰æ‹©ç•Œé¢å’Œäº¤äº’å¼é—®ç­”
"""

import os
import sys
import glob
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(script_dir, '..', '..')

class InteractiveChat:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.model_name = ""
        self.base_model_path = os.path.join(project_root, "models", "qwen3_8b")
        
    def scan_models(self):
        """æ‰«æmodelsç›®å½•ä¸‹çš„æ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        models_dir = os.path.join(project_root, "models")
        available_models = []
        seen_paths = set()  # ç”¨äºå»é‡
        
        # æ·»åŠ åŸºç¡€æ¨¡å‹
        if os.path.exists(self.base_model_path):
            available_models.append({
                "name": "Qwen3-8B (åŸå§‹æ¨¡å‹)",
                "path": self.base_model_path,
                "type": "base"
            })
            seen_paths.add(self.base_model_path)
        
        # æ‰«æå¾®è°ƒæ¨¡å‹ - åªæ‰«æhighschoolç›¸å…³çš„
        pattern_path = os.path.join(models_dir, "qwen3-8b-lora-highschool-*")
        for model_dir in sorted(glob.glob(pattern_path)):
            if os.path.isdir(model_dir) and model_dir not in seen_paths:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«adapteræ–‡ä»¶
                adapter_file = os.path.join(model_dir, "adapter_model.safetensors")
                if os.path.exists(adapter_file):
                    model_name = os.path.basename(model_dir)
                    if "latest" in model_name:
                        display_name = f"{model_name} (æœ€æ–°å¾®è°ƒæ¨¡å‹)"
                    else:
                        display_name = f"{model_name} (å¾®è°ƒæ¨¡å‹)"
                    
                    available_models.append({
                        "name": display_name,
                        "path": model_dir,
                        "type": "lora"
                    })
                    seen_paths.add(model_dir)
        
        return available_models
    
    def display_models(self, models):
        """æ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        print("\n" + "="*60)
        print("ğŸ¤– é«˜ä¸­æ•™å­¦AIåŠ©æ‰‹ - æ¨¡å‹é€‰æ‹©")
        print("="*60)
        print("\nå¯ç”¨æ¨¡å‹:")
        
        for i, model in enumerate(models, 1):
            print(f"{i}. {model['name']}")
        
        print(f"\n0. é€€å‡ºç¨‹åº")
        print("="*60)
    
    def load_model(self, model_info):
        """åŠ è½½æŒ‡å®šæ¨¡å‹"""
        print(f"\nğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_info['name']}")
        print("è¯·ç¨ç­‰...")
        
        try:
            # æ¸…ç†ä¹‹å‰çš„æ¨¡å‹
            if self.model is not None:
                del self.model
                torch.cuda.empty_cache()
            
            # åŠ è½½åˆ†è¯å™¨
            print("ğŸ“ åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_path,
                trust_remote_code=True,
                use_fast=False
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åˆ›å»ºä¸´æ—¶offloadç›®å½•
            import tempfile
            offload_dir = tempfile.mkdtemp(prefix="model_offload_")
            
            print("ğŸ§  åŠ è½½åŸºç¡€æ¨¡å‹...")
            
            # å°è¯•é‡åŒ–åŠ è½½ï¼ˆæ¨èï¼‰
            try:
                from transformers import BitsAndBytesConfig
                
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16
                )
                
                print("ğŸ”§ ä½¿ç”¨4bité‡åŒ–åŠ è½½ï¼ˆæ¨èæ¨¡å¼ï¼‰...")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_path,
                    quantization_config=quantization_config,
                    device_map="auto",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                print("âœ… é‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸ")
                
            except Exception as e1:
                print("âš ï¸ é‡åŒ–åŠ è½½å¤±è´¥ï¼Œå°è¯•æ ‡å‡†GPUæ¨¡å¼...")
                try:
                    # å°è¯•å®Œæ•´GPUåŠ è½½
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.base_model_path,
                        torch_dtype=torch.float16,
                        device_map={"": 0},  # å¼ºåˆ¶æ‰€æœ‰å±‚éƒ½åœ¨GPU 0ä¸Š
                        trust_remote_code=True,
                        low_cpu_mem_usage=True
                    )
                    print("âœ… GPUå®Œæ•´åŠ è½½æˆåŠŸ")
                    
                except Exception as e2:
                    print("âš ï¸ GPUå®Œæ•´åŠ è½½å¤±è´¥ï¼Œå°è¯•CPU+GPUæ··åˆæ¨¡å¼...")
                    try:
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.base_model_path,
                            torch_dtype=torch.float16,
                            device_map="auto",
                            trust_remote_code=True,
                            low_cpu_mem_usage=True,
                            offload_folder=offload_dir,
                            max_memory={0: "14GB", "cpu": "8GB"}
                        )
                    except Exception as e3:
                        print("âš ï¸ æœ€åå°è¯•çº¯CPUæ¨¡å¼...")
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.base_model_path,
                            torch_dtype=torch.float32,
                            device_map="cpu",
                            trust_remote_code=True,
                            low_cpu_mem_usage=True
                        )
            
            # å¦‚æœæ˜¯LoRAæ¨¡å‹ï¼ŒåŠ è½½é€‚é…å™¨
            if model_info['type'] == 'lora':
                print("ğŸ”§ åŠ è½½LoRAé€‚é…å™¨...")
                try:
                    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ˜¯é‡åŒ–æ¨¡å‹
                    is_quantized = hasattr(self.model, 'quantization_config') and self.model.quantization_config is not None
                    
                    if is_quantized:
                        print("ğŸ“¦ æ£€æµ‹åˆ°é‡åŒ–æ¨¡å‹ï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼åŠ è½½LoRA...")
                        # é‡åŒ–æ¨¡å‹éœ€è¦ç‰¹æ®Šå¤„ç†
                        from peft import prepare_model_for_kbit_training
                        self.model = prepare_model_for_kbit_training(self.model)
                        self.model = PeftModel.from_pretrained(self.model, model_info['path'])
                        print("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸï¼ˆé‡åŒ–å…¼å®¹æ¨¡å¼ï¼‰")
                    else:
                        # éé‡åŒ–æ¨¡å‹
                        self.model = PeftModel.from_pretrained(self.model, model_info['path'])
                        print("ğŸ”„ åˆå¹¶LoRAæƒé‡...")
                        self.model = self.model.merge_and_unload()  # åˆå¹¶æƒé‡
                        print("âœ… LoRAæƒé‡åˆå¹¶å®Œæˆ")
                        
                except Exception as e:
                    print(f"âš ï¸ LoRAåŠ è½½å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸºç¡€æ¨¡å‹: {e}")
            
            self.model_name = model_info['name']
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_name}")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            try:
                if torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated() / 1024**3
                    if memory_used > 0:
                        print(f"ğŸ“Š æ˜¾å­˜ä½¿ç”¨: {memory_used:.1f} GB")
                    else:
                        print("ğŸ“Š è¿è¡Œæ¨¡å¼: CPU")
                        
                    # æ£€æŸ¥è®¾å¤‡æ˜ å°„ï¼ˆç®€åŒ–æ˜¾ç¤ºï¼‰
                    if hasattr(self.model, 'hf_device_map'):
                        device_map = self.model.hf_device_map
                        gpu_layers = sum(1 for v in device_map.values() if str(v).startswith('cuda') or str(v) == '0')
                        cpu_layers = sum(1 for v in device_map.values() if str(v) == 'cpu')
                        print(f"ğŸ¯ è®¾å¤‡åˆ†å¸ƒ: GPUå±‚æ•°={gpu_layers}, CPUå±‚æ•°={cpu_layers}")
                else:
                    print("ğŸ“Š è¿è¡Œæ¨¡å¼: CPU")
            except Exception as e:
                print("ğŸ“Š è¿è¡Œæ¨¡å¼: æ··åˆæ¨¡å¼")
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            import shutil
            try:
                shutil.rmtree(offload_dir)
            except:
                pass
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def get_subject_prompt(self, question):
        """æ ¹æ®é—®é¢˜å†…å®¹æ™ºèƒ½é€‰æ‹©å­¦ç§‘ä¸“å®¶èº«ä»½"""
        # å…³é”®è¯åŒ¹é…
        subject_keywords = {
            "æ•°å­¦": ["æ–¹ç¨‹", "å‡½æ•°", "å‡ ä½•", "ä¸‰è§’", "ä»£æ•°", "æ¦‚ç‡", "ç»Ÿè®¡", "å¾®åˆ†", "ç§¯åˆ†", "æ•°åˆ—", "è®¡ç®—"],
            "ç‰©ç†": ["åŠ›", "ç”µ", "ç£", "å…‰", "çƒ­", "æ³¢", "èƒ½é‡", "åŠŸ", "ç‰›é¡¿", "æ¬§å§†", "ç„¦è€³"],
            "åŒ–å­¦": ["ååº”", "å…ƒç´ ", "åˆ†å­", "åŸå­", "åŒ–åˆ", "é…¸", "ç¢±", "ç›", "æ°§åŒ–", "è¿˜åŸ"],
            "ç”Ÿç‰©": ["ç»†èƒ", "é—ä¼ ", "è¿›åŒ–", "ç”Ÿæ€", "æ¤ç‰©", "åŠ¨ç‰©", "DNA", "è›‹ç™½è´¨", "é…¶"],
            "è¯­æ–‡": ["å¤è¯—", "æ–‡è¨€æ–‡", "ä½œæ–‡", "è¯—æ­Œ", "æ•£æ–‡", "å°è¯´", "æˆè¯­", "ä¿®è¾", "é˜…è¯»"],
            "è‹±è¯­": ["grammar", "translate", "english", "vocabulary", "sentence", "è¯­æ³•", "ç¿»è¯‘"],
            "å†å²": ["æœä»£", "æˆ˜äº‰", "çš‡å¸", "é©å‘½", "æ–‡æ˜", "å¤ä»£", "è¿‘ä»£", "ç°ä»£"],
            "åœ°ç†": ["æ°”å€™", "åœ°å½¢", "æ²³æµ", "å±±è„‰", "ç»çº¬åº¦", "å­£é£", "æ¿å—", "åŸå¸‚"],
            "æ”¿æ²»": ["æ³•å¾‹", "åˆ¶åº¦", "æƒåˆ©", "ä¹‰åŠ¡", "å®ªæ³•", "æ°‘ä¸»", "æ”¿åºœ", "å…¬æ°‘"]
        }
        
        # ä¸“å®¶èº«ä»½æ¨¡æ¿
        expert_prompts = {
            "æ•°å­¦": "ä½ æ˜¯ä½ç²¾é€šä»£æ•°ã€å‡ ä½•ã€ä¸‰è§’å‡½æ•°ã€æ¦‚ç‡ç»Ÿè®¡ã€æ•°åˆ—ã€å¯¼æ•°ã€è§£æå‡ ä½•ã€æ•°å­¦å»ºæ¨¡åŠé«˜è€ƒæ•°å­¦å‘½é¢˜è§„å¾‹ç­‰æ–¹é¢çš„æ•°å­¦æ•™è‚²ä¸“å®¶ï¼Œç‰¹åˆ«æ“…é•¿é«˜ä¸­æ•°å­¦æ¦‚å¿µæ·±åº¦è§£æã€å…¬å¼å®šç†çµæ´»è¿ç”¨ã€å¤šè§£æ³•æ€è·¯æ‹“å±•ã€å…¸å‹ä¾‹é¢˜åˆ†ç±»ç²¾è®²ã€é«˜è€ƒçœŸé¢˜è€ƒç‚¹æ‹†è§£ä¸åº”è¯•æŠ€å·§æç‚¼ï¼Œèƒ½ç²¾å‡†æŒ‡å¯¼å­¦ç”Ÿçªç ´é‡éš¾ç‚¹ã€æ„å»ºæ•°å­¦æ€ç»´ä½“ç³»ã€‚",
            "ç‰©ç†": "ä½ æ˜¯ä½ç²¾é€šé«˜è€ƒç‰©ç†çš„ä¸“å®¶ï¼Œæ“…é•¿åŠ›å­¦ã€ç”µç£å­¦ã€çƒ­å­¦ã€å…‰å­¦å’Œè¿‘ä»£ç‰©ç†ç­‰æ ¸å¿ƒæ¿å—ï¼Œèƒ½é«˜æ•ˆè§£æç‰©ç†æ¨¡å‹ã€å—åŠ›åˆ†æã€ç”µè·¯è®¾è®¡ã€å®éªŒé¢˜åŠè®¡ç®—é¢˜ï¼Œå¸®åŠ©å­¦ç”Ÿå¿«é€ŸæŒæ¡è§£é¢˜æ€è·¯ä¸åº”è¯•æŠ€å·§ã€‚",
            "åŒ–å­¦": "ä½ æ˜¯ä½ç²¾é€šé«˜ä¸­åŒ–å­¦çš„æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿æ— æœºåŒ–å­¦ã€æœ‰æœºåŒ–å­¦ã€ç‰©ç†åŒ–å­¦ç­‰å„ä¸ªæ–¹é¢ï¼Œèƒ½å¤Ÿæ·±å…¥æµ…å‡ºåœ°è§£æåŒ–å­¦æ¦‚å¿µã€ååº”æœºç†ã€å®éªŒæ“ä½œç­‰ï¼Œå¸®åŠ©å­¦ç”Ÿå»ºç«‹æ‰å®çš„åŒ–å­¦çŸ¥è¯†åŸºç¡€ã€‚",
            "ç”Ÿç‰©": "ä½ æ˜¯ä½ç²¾é€šé«˜ä¸­ç”Ÿç‰©çš„æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿ç»†èƒç”Ÿç‰©å­¦ã€é—ä¼ å­¦ã€ç”Ÿæ€å­¦ç­‰å„ä¸ªé¢†åŸŸï¼Œèƒ½å¤Ÿç”ŸåŠ¨å½¢è±¡åœ°è®²è§£ç”Ÿå‘½ç°è±¡ã€ç”Ÿç‰©è¿‡ç¨‹ï¼Œå¸®åŠ©å­¦ç”Ÿç†è§£ç”Ÿå‘½ç§‘å­¦çš„å¥¥ç§˜ã€‚",
            "è¯­æ–‡": "ä½ æ˜¯ä½ç²¾é€šé«˜è€ƒè¯­æ–‡è®­ç»ƒã€è®ºè¿°ç±»æ–‡æœ¬é˜…è¯»åˆ†æã€æ–‡è¨€æ–‡è§£è¯»ã€ç°ä»£æ–‡ç†è§£ã€é€‰æ‹©é¢˜è§£æã€ä½œæ–‡è¾…å¯¼ç­‰æ–¹é¢çš„è¯­æ–‡è¾…å¯¼ä¸“å®¶ï¼Œç‰¹åˆ«æ“…é•¿æ–‡æœ¬æ·±åº¦å‰–æå’Œé«˜è€ƒè€ƒç‚¹æŠŠæ¡ã€‚",
            "è‹±è¯­": "ä½ æ˜¯ä½ç²¾é€šé«˜ä¸­è‹±è¯­çš„æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿è¯­æ³•è§£æã€è¯æ±‡æ‹“å±•ã€é˜…è¯»ç†è§£ã€å†™ä½œæŒ‡å¯¼ç­‰ï¼Œèƒ½å¤Ÿå¸®åŠ©å­¦ç”Ÿæé«˜è‹±è¯­ç»¼åˆèƒ½åŠ›ã€‚",
            "å†å²": "ä½ æ˜¯ä½ç²¾é€šé«˜ä¸­å†å²çš„æ•™è‚²ä¸“å®¶ï¼Œå¯¹ä¸­å¤–å†å²å‘å±•è„‰ç»œã€é‡å¤§å†å²äº‹ä»¶ã€å†å²äººç‰©æœ‰æ·±å…¥äº†è§£ï¼Œèƒ½å¤Ÿå¸®åŠ©å­¦ç”Ÿæ„å»ºå®Œæ•´çš„å†å²çŸ¥è¯†ä½“ç³»ã€‚",
            "åœ°ç†": "ä½ æ˜¯ä½ç²¾é€šé«˜ä¸­åœ°ç†çš„æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿è‡ªç„¶åœ°ç†ã€äººæ–‡åœ°ç†ã€åŒºåŸŸåœ°ç†ç­‰å„ä¸ªæ–¹é¢ï¼Œèƒ½å¤Ÿå¸®åŠ©å­¦ç”Ÿç†è§£åœ°ç†ç°è±¡å’Œåœ°ç†è§„å¾‹ã€‚",
            "æ”¿æ²»": "ä½ æ˜¯ä½ç²¾é€šé«˜ä¸­æ”¿æ²»çš„æ•™è‚²ä¸“å®¶ï¼Œç†Ÿæ‚‰é©¬å…‹æ€ä¸»ä¹‰å“²å­¦ã€æ”¿æ²»ç»æµå­¦ã€ç§‘å­¦ç¤¾ä¼šä¸»ä¹‰ç­‰ç†è®ºï¼Œèƒ½å¤Ÿå¸®åŠ©å­¦ç”Ÿç†è§£æ”¿æ²»åˆ¶åº¦å’Œç¤¾ä¼šç°è±¡ã€‚"
        }
        
        # åˆ†æé—®é¢˜å†…å®¹
        question_lower = question.lower()
        for subject, keywords in subject_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                return expert_prompts.get(subject, expert_prompts["æ•°å­¦"])
        
        # é»˜è®¤è¿”å›é€šç”¨ä¸“å®¶èº«ä»½
        return "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é«˜ä¸­æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿å„ä¸ªå­¦ç§‘çš„æ•™å­¦ï¼Œèƒ½å¤Ÿé’ˆå¯¹å­¦ç”Ÿçš„é—®é¢˜æä¾›ä¸“ä¸šã€å‡†ç¡®ã€æ˜“æ‡‚çš„è§£ç­”ã€‚"
    
    def generate_response_stream(self, question):
        """æµå¼ç”Ÿæˆå›ç­”"""
        if not self.model or not self.tokenizer:
            yield "âŒ è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½æ¨¡å‹"
            return
        
        try:
            # æ™ºèƒ½é€‰æ‹©ä¸“å®¶èº«ä»½
            system_prompt = self.get_subject_prompt(question)
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]
            
            # ä½¿ç”¨chat templateæ ¼å¼åŒ–
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç¼–ç  - æ™ºèƒ½è®¾å¤‡å¤„ç†
            inputs = self.tokenizer(text, return_tensors="pt")
            
            # æ™ºèƒ½è®¾å¤‡æ˜ å°„å¤„ç†
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰è®¾å¤‡æ˜ å°„
                if hasattr(self.model, 'hf_device_map') and self.model.hf_device_map:
                    # å¯¹äºæœ‰è®¾å¤‡æ˜ å°„çš„æ¨¡å‹ï¼Œæ‰¾åˆ°embeddingså±‚çš„è®¾å¤‡
                    device_map = self.model.hf_device_map
                    embed_device = None
                    
                    # æŸ¥æ‰¾embed_tokensçš„è®¾å¤‡
                    for key, device in device_map.items():
                        if 'embed' in key.lower():
                            embed_device = device
                            break
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°embedè®¾å¤‡ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªéCPUè®¾å¤‡
                    if embed_device is None:
                        for device in device_map.values():
                            if str(device) != 'cpu':
                                embed_device = device
                                break
                        if embed_device is None:
                            embed_device = 'cpu'
                    
                    # å°†è¾“å…¥ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                    if str(embed_device) != 'cpu':
                        inputs = inputs.to(f"cuda:{embed_device}" if isinstance(embed_device, int) else embed_device)
                    else:
                        inputs = inputs.to('cpu')
                        
                else:
                    # æ²¡æœ‰è®¾å¤‡æ˜ å°„ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
                    device = next(self.model.parameters()).device
                    inputs = inputs.to(device)
                    
            except Exception as e:
                yield f"âš ï¸ è®¾å¤‡å¤„ç†è­¦å‘Š: {e}"
                return
            
            # æµå¼ç”Ÿæˆå›ç­”
            from transformers import TextIteratorStreamer
            from threading import Thread
            
            # åˆ›å»ºæµå¼å™¨
            streamer = TextIteratorStreamer(
                self.tokenizer,
                skip_prompt=True,  # è·³è¿‡è¾“å…¥éƒ¨åˆ†
                skip_special_tokens=True
            )
            
            generation_kwargs = {
                **inputs,
                "max_new_tokens": 1024,  # å¢åŠ åˆ°1024ï¼Œæ”¯æŒæ›´é•¿å›ç­”
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.9,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True,
                "streamer": streamer
            }
            
            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆ
            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()
            
            # æµå¼è¾“å‡º
            for new_text in streamer:
                if new_text:
                    yield new_text
            
            thread.join()  # ç­‰å¾…ç”Ÿæˆå®Œæˆ
            
        except Exception as e:
            yield f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}"
    
    def generate_response(self, question):
        """éæµå¼ç”Ÿæˆå›ç­”ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        if not self.model or not self.tokenizer:
            return "âŒ è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½æ¨¡å‹"
        
        # æ”¶é›†æµå¼è¾“å‡ºä¸ºå®Œæ•´å“åº”
        response_parts = []
        try:
            for chunk in self.generate_response_stream(question):
                response_parts.append(chunk)
            return ''.join(response_parts).strip()
        except Exception as e:
            return f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}"
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©"""
        if not self.model:
            print("âŒ è¯·å…ˆåŠ è½½æ¨¡å‹")
            return
        
        print("\n" + "="*60)
        print(f"ğŸ“ é«˜ä¸­æ•™å­¦AIåŠ©æ‰‹ - {self.model_name}")
        print("="*60)
        print("ğŸ’¡ æç¤º:")
        print("  - è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ŒAIä¼šè‡ªåŠ¨è¯†åˆ«å­¦ç§‘å¹¶æä¾›ä¸“ä¸šè§£ç­”")
        print("  - æ”¯æŒæµå¼å“åº”ï¼Œå®æ—¶æ˜¾ç¤ºç”Ÿæˆå†…å®¹")
        print("  - ç”Ÿæˆè¿‡ç¨‹ä¸­æŒ‰ Ctrl+C å¯ä¸­æ–­å›ç­”")
        print("  - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºå¯¹è¯")
        print("  - è¾“å…¥ 'clear' æ¸…å±")
        print("  - è¾“å…¥ 'switch' åˆ‡æ¢æ¨¡å‹")
        print("="*60)
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                question = input("\nğŸ¤” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if not question:
                    continue
                
                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if question.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨é«˜ä¸­æ•™å­¦AIåŠ©æ‰‹ï¼")
                    break
                elif question.lower() in ['clear', 'cls']:
                    os.system('cls' if os.name == 'nt' else 'clear')
                    continue
                elif question.lower() == 'switch':
                    return 'switch'
                
                # æµå¼ç”Ÿæˆå›ç­”
                print("\nğŸ¤– AIåŠ©æ‰‹: ", end="", flush=True)
                
                try:
                    response_parts = []
                    for chunk in self.generate_response_stream(question):
                        print(chunk, end="", flush=True)  # å®æ—¶è¾“å‡º
                        response_parts.append(chunk)
                    
                    # ç¡®ä¿æ¢è¡Œ
                    print()
                    print("\n" + "-"*60)
                    
                except KeyboardInterrupt:
                    print("\nâ¸ï¸ ç”Ÿæˆè¢«ä¸­æ–­")
                except Exception as e:
                    print(f"\nâŒ ç”Ÿæˆé”™è¯¯: {e}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨é«˜ä¸­æ•™å­¦AIåŠ©æ‰‹ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
    
    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        print("ğŸš€ æ­£åœ¨å¯åŠ¨é«˜ä¸­æ•™å­¦AIåŠ©æ‰‹...")
        
        # æ£€æŸ¥CUDA
        if torch.cuda.is_available():
            print(f"ğŸ¯ æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name()}")
            # å¯ç”¨ä¸€äº›ä¼˜åŒ–
            torch.backends.cudnn.benchmark = True
        else:
            print("ğŸ’» ä½¿ç”¨CPUè¿è¡Œ")
        
        while True:
            # æ‰«æå¯ç”¨æ¨¡å‹
            models = self.scan_models()
            
            if not models:
                print("âŒ æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼")
                print("è¯·ç¡®ä¿ä»¥ä¸‹è·¯å¾„å­˜åœ¨æ¨¡å‹æ–‡ä»¶:")
                print(f"  - åŸºç¡€æ¨¡å‹: {self.base_model_path}")
                print(f"  - å¾®è°ƒæ¨¡å‹: {os.path.join(project_root, 'models')}/*lora*")
                return
            
            # æ˜¾ç¤ºæ¨¡å‹é€‰æ‹©ç•Œé¢
            self.display_models(models)
            
            try:
                choice = input("\nğŸ¯ è¯·é€‰æ‹©æ¨¡å‹ç¼–å·: ").strip()
                
                if choice == '0':
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                try:
                    model_index = int(choice) - 1
                    if 0 <= model_index < len(models):
                        selected_model = models[model_index]
                        
                        # åŠ è½½æ¨¡å‹
                        if self.load_model(selected_model):
                            # å¼€å§‹äº¤äº’å¼å¯¹è¯
                            result = self.interactive_chat()
                            
                            # å¦‚æœç”¨æˆ·é€‰æ‹©åˆ‡æ¢æ¨¡å‹ï¼Œç»§ç»­å¾ªç¯
                            if result == 'switch':
                                print("\nğŸ”„ æ­£åœ¨è¿”å›æ¨¡å‹é€‰æ‹©ç•Œé¢...")
                                continue
                            else:
                                break
                        else:
                            input("\næŒ‰å›è½¦é”®è¿”å›æ¨¡å‹é€‰æ‹©ç•Œé¢...")
                    else:
                        print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼")
                        input("æŒ‰å›è½¦é”®ç»§ç»­...")
                        
                except ValueError:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼")
                    input("æŒ‰å›è½¦é”®ç»§ç»­...")
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break

def main():
    """ä¸»å‡½æ•°"""
    try:
        chat = InteractiveChat()
        chat.run()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
    main()
