#!/usr/bin/env python3
"""
Qwen3-8B QLoRA å¾®è°ƒè„šæœ¬ - é«˜ä¸­å„å­¦ç§‘æ•°æ®é›†ç‰ˆæœ¬
å®Œå…¨ç»•è¿‡accelerateè®¾å¤‡åˆ†å‘é—®é¢˜
"""

import os
import sys
import json
import torch
import logging
from datetime import datetime
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)
from datasets import Dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType

# æ·»åŠ é…ç½®æ–‡ä»¶è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'config'))
from train_config import get_config

import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_data(data_path: str, max_samples=None):
    """åŠ è½½è®­ç»ƒæ•°æ® - é€‚é…é«˜ä¸­å„å­¦ç§‘æ•°æ®é›†æ ¼å¼"""
    logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®: {data_path}")
    
    conversations_list = []
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•ï¼ˆåŒ…å«å¤šä¸ªå­¦ç§‘æ•°æ®é›†ï¼‰
    if os.path.isdir(data_path):
        # åŠ è½½ç›®å½•ä¸­æ‰€æœ‰jsonlæ–‡ä»¶
        import glob
        jsonl_files = glob.glob(os.path.join(data_path, "*.jsonl"))
        logger.info(f"æ‰¾åˆ° {len(jsonl_files)} ä¸ªå­¦ç§‘æ•°æ®é›†")
        
        for jsonl_file in jsonl_files:
            logger.info(f"æ­£åœ¨åŠ è½½: {os.path.basename(jsonl_file)}")
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                file_count = 0
                for line in f:
                    if max_samples and len(conversations_list) >= max_samples:
                        break
                    try:
                        data = json.loads(line.strip())
                        # é€‚é…æ–°æ ¼å¼ï¼šmessages -> conversations
                        if "messages" in data:
                            # å°†messagesè½¬æ¢ä¸ºconversationsæ ¼å¼
                            conversations = []
                            for msg in data["messages"]:
                                conversations.append({
                                    "role": msg["role"],
                                    "content": msg["content"]
                                })
                            conversations_list.append(conversations)
                            file_count += 1
                    except Exception as e:
                        logger.warning(f"è·³è¿‡æ— æ•ˆè¡Œ: {e}")
                        continue
                logger.info(f"ä» {os.path.basename(jsonl_file)} åŠ è½½äº† {file_count} ä¸ªå¯¹è¯")
    else:
        # å•ä¸ªæ–‡ä»¶
        with open(data_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if max_samples and i >= max_samples:
                    break
                try:
                    data = json.loads(line.strip())
                    # å…¼å®¹ä¸¤ç§æ ¼å¼
                    if "conversations" in data:
                        conversations_list.append(data["conversations"])
                    elif "messages" in data:
                        conversations = []
                        for msg in data["messages"]:
                            conversations.append({
                                "role": msg["role"],
                                "content": msg["content"]
                            })
                        conversations_list.append(conversations)
                except Exception as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆè¡Œ: {e}")
                    continue
    
    logger.info(f"æˆåŠŸåŠ è½½ {len(conversations_list)} ä¸ªå¯¹è¯")
    return Dataset.from_dict({"conversations": conversations_list})

def format_conversations(conversations, tokenizer):
    """æ ¼å¼åŒ–å¯¹è¯"""
    text = ""
    for conv in conversations:
        role = conv["role"]
        content = conv["content"]
        if role == "system":
            text += f"<|im_start|>system\n{content}<|im_end|>\n"
        elif role == "user":
            text += f"<|im_start|>user\n{content}<|im_end|>\n"
        elif role == "assistant":
            text += f"<|im_start|>assistant\n{content}<|im_end|>\n"
    
    return text

def preprocess_data(dataset, tokenizer, max_length=2048):
    """é¢„å¤„ç†æ•°æ®"""
    logger.info("æ­£åœ¨é¢„å¤„ç†æ•°æ®...")
    
    def tokenize_function(examples):
        texts = []
        for conversations in examples["conversations"]:
            text = format_conversations(conversations, tokenizer)
            texts.append(text)
        
        # åˆ†è¯
        model_inputs = tokenizer(
            texts,
            truncation=True,
            max_length=max_length,
            padding=False,
            add_special_tokens=False
        )
        
        # labelsä¸input_idsç›¸åŒ
        model_inputs["labels"] = [ids.copy() for ids in model_inputs["input_ids"]]
        
        return model_inputs
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names,
        desc="Tokenizing"
    )
    
    logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œæ•°æ®é›†å¤§å°: {len(tokenized_dataset)}")
    return tokenized_dataset

def main():
    # è·å–å½“å‰è„šæœ¬çš„æ ¹ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.join(script_dir, '..', '..')
    
    # åŠ è½½é…ç½®
    mode = "normal"  # ä» "test" æ”¹ä¸º "normal"
    config = get_config(mode)
    
    # ç”Ÿæˆå¸¦æ—¥æœŸåç¼€çš„è¾“å‡ºç›®å½•
    current_date = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    original_output_dir = config['training']['output_dir']
    # ä½¿ç”¨ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„ - ä½¿ç”¨é«˜ä¸­å„å­¦ç§‘æ•°æ®é›†æ ‡è¯†
    output_dir_with_date = os.path.join(project_root, "models", f"qwen3-8b-lora-highschool-{current_date}")
    config['training']['output_dir'] = output_dir_with_date
    
    # æ›´æ–°å…¶ä»–è·¯å¾„ä¸ºç»å¯¹è·¯å¾„ - ä½¿ç”¨é«˜ä¸­å„å­¦ç§‘æ•°æ®é›†
    config['model']['model_path'] = os.path.join(project_root, "models", "qwen3_8b")  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
    config['data']['train_file'] = os.path.join(project_root, "datasets", "é«˜ä¸­å„å­¦ç§‘jsonlæ•°æ®é›†")
    
    logger.info("=== Qwen3-8B é«˜ä¸­å„å­¦ç§‘æ•°æ®é›† QLoRA å¾®è°ƒå¼€å§‹ ===")
    logger.info(f"æ¨¡å¼: {mode}")
    logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    logger.info(f"æ¨¡å‹è·¯å¾„: {config['model']['model_path']}")
    logger.info(f"é«˜ä¸­å„å­¦ç§‘æ•°æ®ç›®å½•: {config['data']['train_file']}")
    logger.info(f"è¾“å‡ºç›®å½•: {config['training']['output_dir']}")
    logger.info(f"è®­ç»ƒæ—¶é—´: {current_date}")
    
    # æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        logger.error("CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥GPUç¯å¢ƒ")
        return
    
    logger.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
    logger.info(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # æ¸…ç†GPUç¼“å­˜
    torch.cuda.empty_cache()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¦ç”¨accelerateçš„è‡ªåŠ¨è®¾å¤‡æ˜ å°„
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["ACCELERATE_USE_CPU"] = "false"
    
    # é…ç½®é‡åŒ–
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    
    # åŠ è½½åˆ†è¯å™¨
    logger.info("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(
        config['model']['model_path'],
        trust_remote_code=config['model']['trust_remote_code'],
        padding_side="right",
        use_fast=False
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹ - ç»ˆæä¿®å¤ç‰ˆæœ¬
    logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    try:
        # æ–¹æ³•1: å°è¯•ä¸ä½¿ç”¨ä»»ä½•accelerateç›¸å…³å‚æ•°
        import transformers
        old_dispatch = getattr(transformers.modeling_utils, 'dispatch_model', None)
        
        # ä¸´æ—¶ç¦ç”¨dispatch_modelå‡½æ•°
        def dummy_dispatch(model, **kwargs):
            logger.info("è·³è¿‡è®¾å¤‡åˆ†å‘ï¼Œç›´æ¥è¿”å›æ¨¡å‹")
            return model
        
        # æ›¿æ¢dispatch_modelå‡½æ•°
        if old_dispatch:
            transformers.modeling_utils.dispatch_model = dummy_dispatch
            # åŒæ—¶ä¹Ÿæ›¿æ¢accelerateä¸­çš„
            try:
                import accelerate.big_modeling
                accelerate.big_modeling.dispatch_model = dummy_dispatch
            except:
                pass
        
        model = AutoModelForCausalLM.from_pretrained(
            config['model']['model_path'],
            quantization_config=bnb_config,
            trust_remote_code=config['model']['trust_remote_code'],
            torch_dtype=torch.float16
        )
        
        # æ¢å¤åŸå‡½æ•°
        if old_dispatch:
            transformers.modeling_utils.dispatch_model = old_dispatch
            try:
                import accelerate.big_modeling
                accelerate.big_modeling.dispatch_model = old_dispatch
            except:
                pass
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥æ¨¡å‹è®¾å¤‡
        device = next(model.parameters()).device
        logger.info(f"æ¨¡å‹è®¾å¤‡: {device}")
        
        # å¦‚æœæ¨¡å‹åœ¨CPUä¸Šï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°GPUï¼ˆé‡åŒ–æ¨¡å‹åº”è¯¥å·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼‰
        if device.type == 'cpu' and torch.cuda.is_available():
            logger.info("æ£€æµ‹åˆ°æ¨¡å‹åœ¨CPUä¸Šï¼Œè¿™å¯¹äºé‡åŒ–æ¨¡å‹æ˜¯ä¸æ­£å¸¸çš„")
            
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # å‡†å¤‡LoRA
    logger.info("æ­£åœ¨å‡†å¤‡LoRAè®­ç»ƒ...")
    try:
        model = prepare_model_for_kbit_training(model)
        
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=config['lora']['r'],
            lora_alpha=config['lora']['alpha'],
            lora_dropout=config['lora']['dropout'],
            target_modules=config['lora']['target_modules'],
            bias=config['lora']['bias']
        )
        
        model = get_peft_model(model, lora_config)
        logger.info("âœ… LoRAé…ç½®å®Œæˆ")
        model.print_trainable_parameters()
        
    except Exception as e:
        logger.error(f"âŒ LoRAå‡†å¤‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # åŠ è½½æ•°æ®
    try:
        dataset = load_data(
            config['data']['train_file'], 
            config['data']['max_train_samples']
        )
        
        # é¢„å¤„ç†
        train_dataset = preprocess_data(dataset, tokenizer, config['model']['max_length'])
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True,
        return_tensors="pt"
    )
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=config['training']['output_dir'],
        per_device_train_batch_size=config['training']['per_device_train_batch_size'],
        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
        num_train_epochs=config['training']['num_train_epochs'],
        max_steps=config['training']['max_steps'],
        learning_rate=config['training']['learning_rate'],
        warmup_steps=config['training']['warmup_steps'],
        logging_steps=1,  # æ¯æ­¥éƒ½è®°å½•æ—¥å¿—ï¼Œä¾¿äºtensorboardç›‘æ§
        save_steps=config['training']['save_steps'],
        save_total_limit=config['training']['save_total_limit'],
        fp16=config['training']['fp16'],
        dataloader_pin_memory=False,
        remove_unused_columns=config['training']['remove_unused_columns'],
        optim=config['training']['optim'],
        group_by_length=False,
        report_to=["tensorboard"],  # å¯ç”¨tensorboard
        logging_dir=os.path.join(project_root, "logs"),  # tensorboardæ—¥å¿—ç›®å½•
        dataloader_num_workers=0,
        gradient_checkpointing=config['optimization']['use_gradient_checkpointing'],
        save_safetensors=True,
        ddp_find_unused_parameters=False,
        prediction_loss_only=True
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    try:
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=data_collator,
            tokenizer=tokenizer
        )
        logger.info("âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå™¨åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    try:
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {config['training']['output_dir']}")
        
        # åˆ›å»ºæœ€æ–°æ¨¡å‹çš„è½¯é“¾æ¥
        try:
            latest_link = os.path.join(project_root, "models", "qwen3-8b-lora-highschool-latest")
            if os.path.exists(latest_link) or os.path.islink(latest_link):
                os.remove(latest_link)
            
            # åœ¨Windowsä¸Šåˆ›å»ºç›®å½•é“¾æ¥
            if os.name == 'nt':  # Windows
                import subprocess
                subprocess.run(['mklink', '/D', latest_link, config['training']['output_dir']], 
                             shell=True, check=False)
            else:  # Linux/Mac
                os.symlink(config['training']['output_dir'], latest_link)
            
            logger.info(f"ğŸ”— å·²åˆ›å»ºæœ€æ–°æ¨¡å‹é“¾æ¥: {latest_link}")
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ›å»ºè½¯é“¾æ¥å¤±è´¥: {e}")
        
        # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            memory_used = torch.cuda.max_memory_allocated() / 1024**3
            logger.info(f"ğŸ“Š æœ€å¤§æ˜¾å­˜ä½¿ç”¨: {memory_used:.1f} GB")
            
        logger.info("="*50)
        logger.info("ğŸ‰ é«˜ä¸­å„å­¦ç§‘æ•°æ®é›†è®­ç»ƒå®Œæˆæ‘˜è¦:")
        logger.info(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {config['training']['output_dir']}")
        logger.info(f"ğŸ”— æœ€æ–°é“¾æ¥: {os.path.join(project_root, 'models', 'qwen3-8b-lora-highschool-latest')}")
        logger.info(f"â±ï¸ è®­ç»ƒæ—¶é—´: {current_date}")
        logger.info("="*50)
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    finally:
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ GPUç¼“å­˜å·²æ¸…ç†")

if __name__ == "__main__":
    main() 