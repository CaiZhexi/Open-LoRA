#!/usr/bin/env python3
"""
æ¨¡å‹åŠ è½½æµ‹è¯•è„šæœ¬ - è·¯å¾„ä¿®å¤ç‰ˆ
ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ç›¸å¯¹è·¯å¾„ï¼Œå…¼å®¹è·¨å¹³å°
"""

import os
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.join(script_dir, '..', '..')
    return os.path.abspath(project_root)

def main():
    # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ç›¸å¯¹è·¯å¾„
    project_root = get_project_root()
    model_path = os.path.join(project_root, "models", "qwen2-7b-instruct")
    
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶ä½äºæ­£ç¡®ä½ç½®")
        return
    
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ° GPU
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•å¯¹è¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„å®¢æœæœºå™¨äººã€‚"},
            {"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘ä¸‹çš„è®¢å•æ€ä¹ˆè¿˜æ²¡åˆ°ï¼Ÿ"}
        ]
        
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        print("æ­£åœ¨ç”Ÿæˆå›å¤...")
        outputs = model.generate(**inputs, max_new_tokens=100)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print("="*50)
        print("ğŸ¤– æ¨¡å‹å›å¤:")
        print(response)
        print("="*50)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    finally:
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPUç¼“å­˜å·²æ¸…ç†")

if __name__ == "__main__":
    main()
